import os
import json
import shutil
import logging
import asyncio
import random
import aiohttp
from rich.console import Console

# --- DOÄRU IMPORTLAR (DosyalarÄ±na BakÄ±larak DÃ¼zeltildi) ---
from yomi.discovery import MirrorHunter
from yomi.extractors.common import AsyncGenericMangaExtractor
from yomi.utils.archive import create_cbz_archive, create_pdf_document
from yomi.utils.metadata import parse_chapter_metadata

# --- AYARLAR ---
TEST_DIR = "stress_test_output"
SITES_JSON = "sites_test.json"
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s', datefmt='%H:%M:%S')

async def test_single_site(site_key, site_data, hunter, session):
    """
    Tek bir siteyi test eder: URL Bul -> BÃ¶lÃ¼m Listele -> 1-2 BÃ¶lÃ¼m Ä°ndir -> ArÅŸivle
    """
    site_name = site_data['name']
    target_format = random.choice(['cbz', 'pdf'])
    chapter_count = random.randint(1, 2) # HÄ±z iÃ§in az tuttum
    
    print(f"ğŸ‘‰ {site_name} [{target_format.upper()}]...", end=" ", flush=True)

    try:
        # 1. AVCI: URL'Ä° BUL (Async)
        base_domain = site_data.get('base_domain')
        test_path = site_data.get('test_path', "/")
        
        active_url = await hunter.find_active_mirror(base_domain, test_path)
        
        if not active_url:
            print("âŒ URL Yok (Site Ã–lÃ¼)")
            return False

        # 2. EXTRACTOR: BÃ–LÃœMLERÄ° Ã‡EK
        # AsyncGenericMangaExtractor, session ile baÅŸlatÄ±lÄ±r
        extractor = AsyncGenericMangaExtractor(session)
        chapters = await extractor.get_chapters(active_url)

        if not chapters:
            print("âŒ BÃ¶lÃ¼m Listesi BoÅŸ")
            return False

        # 3. Ä°ÅLEM: RASTGELE BÃ–LÃœMLERÄ° Ä°NDÄ°R
        # Listenin baÅŸÄ±ndan (en yeni) bÃ¶lÃ¼mleri alÄ±yoruz ki silinmiÅŸ olma ihtimali dÃ¼ÅŸÃ¼k olsun
        targets = chapters[:chapter_count]
        
        success_count = 0
        manga_clean = "".join([c for c in site_name if c.isalnum() or c in (' ', '-', '_')]).strip()
        site_dir = os.path.join(TEST_DIR, manga_clean)
        os.makedirs(site_dir, exist_ok=True)

        for chapter in targets:
            try:
                # Metadata HazÄ±rla
                meta = parse_chapter_metadata(chapter['title'], site_name, chapter['url'])
                chap_clean = "".join([c for c in chapter['title'] if c.isalnum() or c in (' ', '-', '_')]).strip()
                chap_dir = os.path.join(site_dir, chap_clean)
                os.makedirs(chap_dir, exist_ok=True)

                # SayfalarÄ± Bul
                pages = await extractor.get_pages(chapter['url'])
                if not pages:
                    continue

                # Resimleri Ä°ndir (Parallel)
                download_tasks = []
                for idx, img_url in enumerate(pages):
                    ext = "jpg"
                    if ".png" in img_url.lower(): ext = "png"
                    elif ".webp" in img_url.lower(): ext = "webp"
                    
                    save_path = os.path.join(chap_dir, f"{idx+1:03d}.{ext}")
                    # extractor.download_image metodunu kullanÄ±yoruz
                    download_tasks.append(extractor.download_image(img_url, save_path))
                
                # Hepsini indir
                await asyncio.gather(*download_tasks)

                # ArÅŸivle (PDF veya CBZ)
                archive_success = False
                if target_format == 'pdf':
                    pdf_path = os.path.join(site_dir, f"{chap_clean}.pdf")
                    if create_pdf_document(chap_dir, pdf_path):
                        archive_success = True
                else: # cbz
                    cbz_path = os.path.join(site_dir, f"{chap_clean}.cbz")
                    if create_cbz_archive(chap_dir, cbz_path, meta):
                        archive_success = True
                
                if archive_success:
                    shutil.rmtree(chap_dir) # KlasÃ¶rÃ¼ temizle
                    success_count += 1
            except Exception:
                pass # Tekil bÃ¶lÃ¼m hatasÄ± tÃ¼m testi yakmasÄ±n
        
        if success_count > 0:
            print(f"âœ… PASS ({success_count}/{len(targets)} Ä°ndi)")
            return True
        else:
            print("âŒ Ä°ndirme BaÅŸarÄ±sÄ±z")
            return False

    except Exception as e:
        print(f"âŒ KRÄ°TÄ°K HATA: {str(e)[:50]}")
        return False

async def main():
    # 0. Dosya KontrolÃ¼
    if not os.path.exists(SITES_JSON):
        print(f"âŒ '{SITES_JSON}' bulunamadÄ±! Ã–nce 'python yomi/utils/auto_discovery.py' Ã§alÄ±ÅŸtÄ±r.")
        return

    with open(SITES_JSON, 'r', encoding='utf-8') as f:
        sites = json.load(f)

    # Temizlik
    if os.path.exists(TEST_DIR):
        try: shutil.rmtree(TEST_DIR)
        except: pass
    os.makedirs(TEST_DIR)

    print(f"ğŸ”¥ MASS TEST BAÅLIYOR: {len(sites)} Site")
    print("-" * 50)
    
    # 1. AvcÄ±yÄ± BaÅŸlat
    hunter = MirrorHunter()
    
    # 2. Session BaÅŸlat (TÃ¼m iÅŸlemler iÃ§in tek session, performans artÄ±rÄ±r)
    connector = aiohttp.TCPConnector(limit=20) # AynÄ± anda 20 baÄŸlantÄ±
    async with aiohttp.ClientSession(connector=connector) as session:
        passed = 0
        failed = 0
        
        for i, (key, data) in enumerate(sites.items(), 1):
            print(f"[{i}/{len(sites)}]", end=" ")
            res = await test_single_site(key, data, hunter, session)
            if res: passed += 1
            else: failed += 1

    print("-" * 50)
    print(f"ğŸ“Š SONUÃ‡: {passed} BaÅŸarÄ±lÄ± / {failed} HatalÄ±")
    print(f"âœ… BaÅŸarÄ± OranÄ±: %{(passed/len(sites))*100:.1f}")
    print(f"ğŸ“‚ Ã‡Ä±ktÄ± KlasÃ¶rÃ¼: {TEST_DIR}")

if __name__ == "__main__":
    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())